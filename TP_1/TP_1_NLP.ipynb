{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UQIkxrZ5fTiA"
   },
   "source": [
    "# Instalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 24769,
     "status": "ok",
     "timestamp": 1730333269441,
     "user": {
      "displayName": "Marcela Flaibani",
      "userId": "06388299677795656548"
     },
     "user_tz": 180
    },
    "id": "EVhJz_m2DMH5",
    "outputId": "b74a5b47-33c7-48ef-b89f-91aa0ad1f580"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\flaib\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.28.1)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\flaib\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (4.11.1)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\flaib\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\flaib\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\flaib\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests) (1.26.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\flaib\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests) (2022.6.15.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\flaib\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from beautifulsoup4) (2.3.2.post1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: spacy in c:\\users\\flaib\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (3.8.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\flaib\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\flaib\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\flaib\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\flaib\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\flaib\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.0 in c:\\users\\flaib\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (8.3.2)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\flaib\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\flaib\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\flaib\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in c:\\users\\flaib\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in c:\\users\\flaib\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (0.12.5)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\flaib\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (4.64.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\flaib\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (2.28.1)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\flaib\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (2.9.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\flaib\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (3.1.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\flaib\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (63.2.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\flaib\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (24.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\flaib\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (3.4.1)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\flaib\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (1.24.4)\n",
      "Requirement already satisfied: language-data>=1.2 in c:\\users\\flaib\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\flaib\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in c:\\users\\flaib\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.23.4)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\flaib\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\flaib\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\flaib\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\flaib\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\flaib\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.6.15.1)\n",
      "Requirement already satisfied: blis<1.1.0,>=1.0.0 in c:\\users\\flaib\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from thinc<8.4.0,>=8.3.0->spacy) (1.0.1)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\flaib\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from thinc<8.4.0,>=8.3.0->spacy) (0.1.5)\n",
      "Collecting numpy>=1.19.0 (from spacy)\n",
      "  Using cached numpy-2.0.2-cp310-cp310-win_amd64.whl.metadata (59 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\flaib\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\flaib\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.3)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\flaib\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\flaib\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in c:\\users\\flaib\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.20.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in c:\\users\\flaib\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.0.5)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\flaib\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jinja2->spacy) (2.1.1)\n",
      "Requirement already satisfied: marisa-trie>=0.7.7 in c:\\users\\flaib\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\flaib\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\flaib\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.13.0)\n",
      "Requirement already satisfied: wrapt in c:\\users\\flaib\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.14.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\flaib\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
      "Using cached numpy-2.0.2-cp310-cp310-win_amd64.whl (15.9 MB)\n",
      "Installing collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.24.4\n",
      "    Uninstalling numpy-1.24.4:\n",
      "      Successfully uninstalled numpy-1.24.4\n",
      "Successfully installed numpy-2.0.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\flaib\\AppData\\Local\\Programs\\Python\\Python310\\Lib\\site-packages\\~~mpy'.\n",
      "  You can safely remove it manually.\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "scipy 1.9.1 requires numpy<1.25.0,>=1.18.5, but you have numpy 2.0.2 which is incompatible.\n",
      "tb-nightly 2.10.0a20220811 requires protobuf<3.20,>=3.9.2, but you have protobuf 3.20.0 which is incompatible.\n",
      "tensorflow-model-optimization 0.7.3 requires numpy~=1.14, but you have numpy 2.0.2 which is incompatible.\n",
      "tensorflowjs 3.19.0 requires packaging~=20.9, but you have packaging 24.1 which is incompatible.\n",
      "tf-nightly 2.10.0.dev20220612 requires protobuf<3.20,>=3.9.2, but you have protobuf 3.20.0 which is incompatible.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "blis 1.0.1 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.24.4 which is incompatible.\n",
      "tb-nightly 2.10.0a20220811 requires protobuf<3.20,>=3.9.2, but you have protobuf 3.20.0 which is incompatible.\n",
      "tensorflowjs 3.19.0 requires packaging~=20.9, but you have packaging 24.1 which is incompatible.\n",
      "tf-nightly 2.10.0.dev20220612 requires protobuf<3.20,>=3.9.2, but you have protobuf 3.20.0 which is incompatible.\n",
      "thinc 8.3.2 requires numpy<2.1.0,>=2.0.0; python_version >= \"3.9\", but you have numpy 1.24.4 which is incompatible.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\flaib\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (4.46.1)\n",
      "Requirement already satisfied: sentence_transformers in c:\\users\\flaib\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (3.2.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\flaib\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\\users\\flaib\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (0.26.2)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\flaib\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\flaib\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\flaib\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\flaib\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (2022.9.11)\n",
      "Requirement already satisfied: requests in c:\\users\\flaib\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (2.28.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\flaib\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in c:\\users\\flaib\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (0.20.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\flaib\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\flaib\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sentence_transformers) (2.5.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\flaib\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sentence_transformers) (1.5.2)\n",
      "Requirement already satisfied: scipy in c:\\users\\flaib\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sentence_transformers) (1.9.1)\n",
      "Requirement already satisfied: Pillow in c:\\users\\flaib\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sentence_transformers) (9.2.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\flaib\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\flaib\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\flaib\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch>=1.11.0->sentence_transformers) (2.8.8)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\flaib\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch>=1.11.0->sentence_transformers) (3.1.2)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\flaib\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch>=1.11.0->sentence_transformers) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\flaib\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sympy==1.13.1->torch>=1.11.0->sentence_transformers) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\flaib\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\flaib\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\flaib\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\flaib\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (1.26.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\flaib\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (2022.6.15.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\flaib\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn->sentence_transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\flaib\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn->sentence_transformers) (3.1.0)\n",
      "Collecting numpy>=1.17 (from transformers)\n",
      "  Using cached numpy-1.24.4-cp310-cp310-win_amd64.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\flaib\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence_transformers) (2.1.1)\n",
      "Using cached numpy-1.24.4-cp310-cp310-win_amd64.whl (14.8 MB)\n",
      "Installing collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.0.2\n",
      "    Uninstalling numpy-2.0.2:\n",
      "      Successfully uninstalled numpy-2.0.2\n",
      "Successfully installed numpy-1.24.4\n",
      "Requirement already satisfied: nltk in c:\\users\\flaib\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: click in c:\\users\\flaib\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (8.1.3)\n",
      "Requirement already satisfied: joblib in c:\\users\\flaib\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\flaib\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (2022.9.11)\n",
      "Requirement already satisfied: tqdm in c:\\users\\flaib\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (4.64.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\flaib\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install requests beautifulsoup4\n",
    "%pip install spacy\n",
    "%pip install transformers sentence_transformers\n",
    "%pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting es-core-news-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_sm-3.8.0/es_core_news_sm-3.8.0-py3-none-any.whl (12.9 MB)\n",
      "     ---------------------------------------- 12.9/12.9 MB 5.9 MB/s eta 0:00:00\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('es_core_news_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download es_core_news_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U65MwcrBfPzo"
   },
   "source": [
    "# Librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 321,
     "status": "ok",
     "timestamp": 1730334253334,
     "user": {
      "displayName": "Marcela Flaibani",
      "userId": "06388299677795656548"
     },
     "user_tz": 180
    },
    "id": "TtPNQe9SQJRY",
    "outputId": "ceb533d7-9713-4ff8-ddd9-47ca75ff1f4c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\flaib\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# from google.colab import drive\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "import re\n",
    "import unicodedata\n",
    "# import spacy\n",
    "import es_core_news_sm\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "import joblib\n",
    "from joblib import dump, load\n",
    "from io import BytesIO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rrxpDK7Ae6iB"
   },
   "source": [
    "# Scrapping de Libros del Proyecto Gutenberg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PqYVuOgBTYGc"
   },
   "outputs": [],
   "source": [
    "# URL de la página de los libros populares de Gutenberg\n",
    "url = 'https://www.gutenberg.org/browse/scores/top1000.php#books-last1'\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Extraer el listado de enlaces de libros\n",
    "book_links = []\n",
    "book_list = soup.select('ol li a')  # Selector para los elementos de libros en la lista\n",
    "for book in book_list:\n",
    "    link = 'https://www.gutenberg.org' + book['href']\n",
    "    book_links.append(link)\n",
    "\n",
    "# Limitar a los primeros 1000 (por si hay más en la página)\n",
    "book_links = book_links[:1000]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wHEO1hAYTarG"
   },
   "outputs": [],
   "source": [
    "def get_book_details(book_url):\n",
    "    response = requests.get(book_url)\n",
    "    book_soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Extraer detalles específicos\n",
    "    try:\n",
    "        title = book_soup.select_one('tr:has(th:contains(\"Title\")) td').get_text(strip=True)\n",
    "    except AttributeError:\n",
    "        title = \"N/A\"\n",
    "\n",
    "    try:\n",
    "        summary = book_soup.select_one('tr:has(th:contains(\"Summary\")) td').get_text(strip=True)\n",
    "    except AttributeError:\n",
    "        summary = \"N/A\"\n",
    "\n",
    "    try:\n",
    "        author = book_soup.select_one('tr:has(th:contains(\"Author\")) td').get_text(strip=True)\n",
    "    except AttributeError:\n",
    "        author = \"N/A\"\n",
    "\n",
    "    try:\n",
    "        language = book_soup.select_one('tr:has(th:contains(\"Language\")) td').get_text(strip=True)\n",
    "    except AttributeError:\n",
    "        language = \"N/A\"\n",
    "\n",
    "    # Recoger todos los temas\n",
    "    subjects = []\n",
    "    for subject in book_soup.select('tr:has(th:contains(\"Subject\")) td a'):\n",
    "        subjects.append(subject.get_text(strip=True))\n",
    "\n",
    "    subjects = ', '.join(subjects) if subjects else \"N/A\"\n",
    "\n",
    "    try:\n",
    "        release_date = book_soup.select_one('tr:has(th:contains(\"Release Date\")) td').get_text(strip=True)\n",
    "    except AttributeError:\n",
    "        release_date = \"N/A\"\n",
    "\n",
    "    try:\n",
    "        downloads = book_soup.select_one('tr:has(th:contains(\"Downloads\")) td').get_text(strip=True)\n",
    "    except AttributeError:\n",
    "        downloads = \"N/A\"\n",
    "\n",
    "    # Devolver un diccionario con los detalles del libro\n",
    "    return {\n",
    "        'Title': title,\n",
    "        'Author': author,\n",
    "        'Summary': summary,\n",
    "        'Language': language,\n",
    "        'Subjects': subjects,\n",
    "        'Release Date': release_date,\n",
    "        'Downloads': downloads,\n",
    "        'URL': book_url\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 275
    },
    "executionInfo": {
     "elapsed": 75233,
     "status": "error",
     "timestamp": 1730007965460,
     "user": {
      "displayName": "Marcela Flaibani",
      "userId": "06388299677795656548"
     },
     "user_tz": 180
    },
    "id": "74icpGPlTg_p",
    "outputId": "ec45ea5e-9f3c-4852-ae94-f23a2ec76e04"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/soupsieve/css_parser.py:862: FutureWarning: The pseudo class ':contains' is deprecated, ':-soup-contains' should be used moving forward.\n",
      "  warnings.warn(  # noqa: B028\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-114-3d8a802986d6>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mbook_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_book_details\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlink\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mbooks_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbook_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Pausa para evitar sobrecargar el servidor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Convertir a DataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Crear una lista para almacenar los datos de cada libro\n",
    "books_data = []\n",
    "\n",
    "# Recorrer cada enlace de libro y extraer sus detalles\n",
    "for link in book_links:\n",
    "    book_data = get_book_details(link)\n",
    "    books_data.append(book_data)\n",
    "    time.sleep(1)  # Pausa para evitar sobrecargar el servidor\n",
    "\n",
    "# Convertir a DataFrame\n",
    "books_df = pd.DataFrame(books_data)\n",
    "\n",
    "books_df\n",
    "\n",
    "# Guardar en un archivo CSV\n",
    "# books_df.to_csv('gutenberg_books_detailed.csv', index=False)\n",
    "# print(\"Archivo CSV guardado exitosamente.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_PLrJ7okbmT1"
   },
   "outputs": [],
   "source": [
    "# Guardar en un archivo CSV\n",
    "books_df.to_csv('gutenberg_books_detailed.csv', index=False)\n",
    "print(\"Archivo CSV guardado exitosamente.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RbNUHH5ofjNv"
   },
   "source": [
    "# Funciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1730333334947,
     "user": {
      "displayName": "Marcela Flaibani",
      "userId": "06388299677795656548"
     },
     "user_tz": 180
    },
    "id": "DusRdd5Pehmx"
   },
   "outputs": [],
   "source": [
    "stopwords = [\n",
    "  'de', 'la', 'que', 'el', 'en', 'y', 'a', 'los', 'del', 'se', 'las', 'por', 'un',\n",
    "  'para', 'con', 'no', 'una', 'su', 'al', 'lo', 'como', 'más', 'pero', 'sus', 'le',\n",
    "  'ya', 'o', 'este', 'sí', 'porque', 'esta', 'entre', 'cuando', 'muy', 'sin',\n",
    "  'sobre', 'también', 'me', 'hasta', 'hay', 'donde', 'quien', 'desde', 'todo',\n",
    "  'nos', 'durante', 'todos', 'uno', 'les', 'ni', 'contra', 'otros', 'ese', 'eso',\n",
    "  'ante', 'ellos', 'e', 'esto', 'mí', 'antes', 'algunos', 'qué', 'unos', 'yo',\n",
    "  'otro', 'otras', 'otra', 'él', 'tanto', 'esa', 'estos', 'mucho', 'quienes',\n",
    "  'nada', 'muchos', 'cual', 'poco', 'ella', 'estar', 'estas', 'algunas', 'algo',\n",
    "  'nosotros', 'mi', 'mis', 'tú', 'te', 'ti', 'tu', 'tus', 'ellas', 'nosotras',\n",
    "  'vosotros', 'vosotras', 'os', 'mío', 'mía', 'míos', 'mías', 'tuyo', 'tuya',\n",
    "  'tuyos', 'tuyas', 'suyo', 'suya', 'suyos', 'suyas', 'nuestro', 'nuestra',\n",
    "  'nuestros', 'nuestras', 'vuestro', 'vuestra', 'vuestros', 'vuestras', 'es'\n",
    "]\n",
    "\n",
    "def eliminate_punctuation(text):\n",
    "  text = re.sub(r'[^\\w\\s]','',text)\n",
    "  return text\n",
    "\n",
    "def eliminate_stopwords(text):\n",
    "  palabras = text.split()\n",
    "  palabras_filtradas = [palabra for palabra in palabras if palabra.lower() not in stopwords]\n",
    "  text = ' '.join(palabras_filtradas)\n",
    "  return text\n",
    "\n",
    "def to_lower(text):\n",
    "  text = text.lower()\n",
    "  return text\n",
    "\n",
    "def eliminate_accents(text):\n",
    "  text = unicodedata.normalize('NFKD', text).encode('ASCII', 'ignore').decode('utf-8')\n",
    "  return text\n",
    "\n",
    "def tokenizacion(text):\n",
    "  nlp = es_core_news_sm.load()\n",
    "  doc = nlp(text)\n",
    "  tokens = [token.text for token in doc]\n",
    "  return tokens\n",
    "\n",
    "# def preprocess_text(text):\n",
    "#     text = to_lower(text)\n",
    "#     text = eliminate_accents(text)\n",
    "#     text = eliminate_punctuation(text)\n",
    "#     # text = eliminate_stopwords(text)\n",
    "#     return text\n",
    "\n",
    "# Define el pipeline\n",
    "def pipeline(text, functions):\n",
    "    for func in functions:\n",
    "        text = func(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TpqLUl1nfmbA"
   },
   "source": [
    "# Entrenamiento del Clasificador de Estado de Ánimo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TbUgsyB6Xu_x"
   },
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1730333334947,
     "user": {
      "displayName": "Marcela Flaibani",
      "userId": "06388299677795656548"
     },
     "user_tz": 180
    },
    "id": "vHuJ6QKNOzvO"
   },
   "outputs": [],
   "source": [
    "# Definimos las etiquetas\n",
    "labels = [(0, \"Alegre\"),(1, \"Neutral\"), (2, \"Triste\")]\n",
    "dataset = []\n",
    "\n",
    "# Textos de \"Alegre\"\n",
    "dataset.extend([\n",
    "    (0, \"Hoy es el mejor día de mi vida.\"),\n",
    "    (0, \"No puedo dejar de sonreír.\"),\n",
    "    (0, \"Todo parece estar saliendo perfecto.\"),\n",
    "    (0, \"Me siento increíblemente bien.\"),\n",
    "    (0, \"La vida es maravillosa.\"),\n",
    "    (0, \"Hoy me desperté con mucha energía.\"),\n",
    "    (0, \"Estoy lleno de esperanza y alegría.\"),\n",
    "    (0, \"¡Es un día para celebrar!\"),\n",
    "    (0, \"Siento que nada puede detenerme.\"),\n",
    "    (0, \"Estoy muy agradecido por todo.\"),\n",
    "    (0, \"Me siento lleno de vida.\"),\n",
    "    (0, \"No podría pedir nada mejor.\"),\n",
    "    (0, \"Todo me hace reír hoy.\"),\n",
    "    (0, \"Me siento ligero y feliz.\"),\n",
    "    (0, \"Qué bonito es vivir.\"),\n",
    "    (0, \"Estoy tan emocionado por el futuro.\"),\n",
    "    (0, \"Las cosas buenas están por venir.\"),\n",
    "    (0, \"Hoy siento una paz increíble.\"),\n",
    "    (0, \"Nada podría arruinar este día.\"),\n",
    "    (0, \"Estoy en la cima del mundo.\"),\n",
    "    (0, \"Siento que todo es posible.\"),\n",
    "    (0, \"Hoy todo brilla más.\"),\n",
    "    (0, \"Estoy lleno de buenas vibras.\"),\n",
    "    (0, \"La vida es más bella de lo que imaginaba.\"),\n",
    "    (0, \"Hoy nada me preocupa.\"),\n",
    "    (0, \"Me siento en armonía con el mundo.\"),\n",
    "    (0, \"Me encanta todo lo que me rodea.\"),\n",
    "    (0, \"Qué día tan espectacular.\"),\n",
    "    (0, \"Estoy en el mejor momento de mi vida.\"),\n",
    "    (0, \"No hay palabras para describir lo feliz que estoy.\"),\n",
    "    (0, \"Todo se siente como un sueño increíble.\"),\n",
    "    (0, \"Hoy me siento más vivo que nunca.\"),\n",
    "    (0, \"La felicidad me inunda por completo.\"),\n",
    "    (0, \"Cada momento parece mágico.\"),\n",
    "    (0, \"Me siento lleno de amor y alegría.\"),\n",
    "    (0, \"Hoy me siento imparable.\"),\n",
    "    (0, \"Todo es posible con esta actitud.\"),\n",
    "    (0, \"Es un día lleno de sorpresas.\"),\n",
    "    (0, \"Estoy lleno de gratitud.\"),\n",
    "    (0, \"Mis sueños parecen más cerca que nunca.\"),\n",
    "    (0, \"Siento la energía fluir en mí.\"),\n",
    "    (0, \"Cada pequeño detalle me hace sonreír.\"),\n",
    "    (0, \"Hoy todo es color de rosa.\"),\n",
    "    (0, \"No hay nada que empañe mi felicidad.\"),\n",
    "    (0, \"Tengo ganas de cantar de felicidad.\"),\n",
    "    (0, \"Hoy veo la vida con otros ojos.\"),\n",
    "    (0, \"Estoy completamente en paz.\"),\n",
    "    (0, \"Nada puede detenerme hoy.\"),\n",
    "    (0, \"Estoy disfrutando cada segundo.\"),\n",
    "    (0, \"Hoy todo tiene sentido.\"),\n",
    "    (0, \"Siento una alegría inexplicable.\"),\n",
    "    (0, \"Me siento más fuerte y feliz que nunca.\"),\n",
    "    (0, \"La felicidad es contagiosa.\"),\n",
    "    (0, \"Hoy todo parece posible.\"),\n",
    "    (0, \"Estoy rodeado de buena energía.\"),\n",
    "    (0, \"Todo está a mi favor.\"),\n",
    "    (0, \"Hoy no hay lugar para la tristeza.\"),\n",
    "    (0, \"Me siento pleno y satisfecho.\"),\n",
    "    (0, \"Hoy cada sonrisa es sincera.\"),\n",
    "    (0, \"Nada podría hacerme más feliz.\"),\n",
    "    (0, \"Me siento afortunado de estar vivo.\"),\n",
    "    (0, \"Hoy es un día especial.\"),\n",
    "    (0, \"El mundo parece brillar más hoy.\"),\n",
    "    (0, \"Hoy soy la mejor versión de mí mismo.\"),\n",
    "    (0, \"Me siento más vivo que nunca.\"),\n",
    "    (0, \"Es un día perfecto en todos los sentidos.\"),\n",
    "    (0, \"Siento que puedo lograr cualquier cosa.\"),\n",
    "    (0, \"Hoy el mundo es un lugar perfecto.\"),\n",
    "    (0, \"La vida tiene más sentido hoy.\"),\n",
    "    (0, \"Hoy me siento querido y valorado.\"),\n",
    "    (0, \"No hay nada que cambiaría de este día.\"),\n",
    "    (0, \"Siento que todo está en su lugar.\"),\n",
    "    (0, \"Hoy todo tiene un toque de magia.\"),\n",
    "    (0, \"Me siento feliz y en paz.\"),\n",
    "    (0, \"Cada momento es un regalo.\"),\n",
    "    (0, \"La felicidad me envuelve hoy.\"),\n",
    "    (0, \"Hoy tengo una gran paz interior.\"),\n",
    "    (0, \"Siento que nada puede afectarme.\"),\n",
    "    (0, \"Hoy soy dueño de mi felicidad.\"),\n",
    "    (0, \"Todo lo que veo me hace feliz.\"),\n",
    "    (0, \"Estoy lleno de optimismo.\"),\n",
    "    (0, \"Siento que el universo conspira a mi favor.\"),\n",
    "    (0, \"Cada segundo es precioso hoy.\"),\n",
    "    (0, \"No hay lugar para la negatividad.\"),\n",
    "    (0, \"Hoy todo está en armonía.\"),\n",
    "    (0, \"Estoy en total paz conmigo mismo.\"),\n",
    "    (0, \"El mundo es maravilloso.\"),\n",
    "    (0, \"Cada detalle me llena de alegría.\"),\n",
    "    (0, \"Hoy tengo mucho por agradecer.\"),\n",
    "    (0, \"Me siento imparable.\"),\n",
    "    (0, \"Hoy veo belleza en todo.\"),\n",
    "    (0, \"Estoy agradecido por este día.\"),\n",
    "    (0, \"Hoy todo es alegría y amor.\"),\n",
    "    (0, \"Cada instante es perfecto.\"),\n",
    "    (0, \"Estoy rodeado de amor y felicidad.\"),\n",
    "    (0, \"Me siento completamente feliz.\"),\n",
    "    (0, \"Hoy todo sale a la perfección.\"),\n",
    "    (0, \"Tengo un corazón lleno de alegría.\"),\n",
    "    (0, \"Estoy satisfecho con todo lo que tengo.\"),\n",
    "    (0, \"Hoy tengo ganas de vivir.\"),\n",
    "    (0, \"El día está lleno de posibilidades.\"),\n",
    "    (0, \"Me siento renovado y feliz.\"),\n",
    "    (0, \"Cada respiración es un regalo.\"),\n",
    "    (0, \"Me siento en el lugar correcto.\"),\n",
    "    (0, \"Hoy todo se alinea a mi favor.\"),\n",
    "    (0, \"No puedo pedir más de la vida.\"),\n",
    "    (0, \"Me siento alegre y afortunado.\"),\n",
    "    (0, \"Todo lo que me rodea es felicidad.\")\n",
    "])\n",
    "\n",
    "# Textos de \"Neutral\"\n",
    "dataset.extend([\n",
    "    (1, \"Hoy es un día como cualquier otro.\"),\n",
    "    (1, \"No me siento ni bien ni mal.\"),\n",
    "    (1, \"Todo está en calma.\"),\n",
    "    (1, \"La vida sigue igual que siempre.\"),\n",
    "    (1, \"No hay mucho que destacar hoy.\"),\n",
    "    (1, \"Me siento bastante normal.\"),\n",
    "    (1, \"No tengo emociones fuertes hoy.\"),\n",
    "    (1, \"Todo está bajo control.\"),\n",
    "    (1, \"Estoy en mi punto medio.\"),\n",
    "    (1, \"Es solo un día más.\"),\n",
    "    (1, \"Las cosas están bien, nada especial.\"),\n",
    "    (1, \"Hoy no tengo mucho que decir.\"),\n",
    "    (1, \"Todo está en su lugar.\"),\n",
    "    (1, \"No tengo ninguna preocupación hoy.\"),\n",
    "    (1, \"Siento que todo está balanceado.\"),\n",
    "    (1, \"Solo estoy pasando el día.\"),\n",
    "    (1, \"Nada fuera de lo común ha ocurrido.\"),\n",
    "    (1, \"Hoy es un día promedio.\"),\n",
    "    (1, \"No hay mucho que reportar.\"),\n",
    "    (1, \"Todo está tranquilo.\"),\n",
    "    (1, \"Me siento estable.\"),\n",
    "    (1, \"Hoy todo está en su sitio.\"),\n",
    "    (1, \"La vida sigue su curso.\"),\n",
    "    (1, \"No tengo ninguna queja ni felicidad extrema.\"),\n",
    "    (1, \"Hoy es un día más.\"),\n",
    "    (1, \"Estoy en paz, nada más.\"),\n",
    "    (1, \"Todo parece rutinario hoy.\"),\n",
    "    (1, \"No tengo grandes emociones hoy.\"),\n",
    "    (1, \"Es un día neutral.\"),\n",
    "    (1, \"Siento que todo está en orden.\"),\n",
    "    (1, \"El día pasa sin muchas novedades.\"),\n",
    "    (1, \"Hoy no tengo prisa ni pausa.\"),\n",
    "    (1, \"Es un día bastante ordinario.\"),\n",
    "    (1, \"Sigo mi rutina habitual.\"),\n",
    "    (1, \"Todo parece muy normal hoy.\"),\n",
    "    (1, \"No tengo ninguna expectativa especial.\"),\n",
    "    (1, \"Hoy me siento en equilibrio.\"),\n",
    "    (1, \"Es solo otro día cualquiera.\"),\n",
    "    (1, \"No hay nada de especial en este día.\"),\n",
    "    (1, \"Siento que todo sigue igual.\"),\n",
    "    (1, \"Nada fuera de lo común ha pasado.\"),\n",
    "    (1, \"Todo sigue su curso normal.\"),\n",
    "    (1, \"Es un día sin altibajos.\"),\n",
    "    (1, \"No espero grandes cosas hoy.\"),\n",
    "    (1, \"Mi estado de ánimo está estable.\"),\n",
    "    (1, \"No tengo nada importante que contar.\"),\n",
    "    (1, \"Hoy me siento neutral.\"),\n",
    "    (1, \"El día avanza sin más.\"),\n",
    "    (1, \"Es un día tranquilo.\"),\n",
    "    (1, \"No tengo emociones destacables hoy.\"),\n",
    "    (1, \"Todo está en calma.\"),\n",
    "    (1, \"Es un día sin eventos.\"),\n",
    "    (1, \"La jornada transcurre como siempre.\"),\n",
    "    (1, \"No hay nada que sobresalga hoy.\"),\n",
    "    (1, \"Me siento equilibrado.\"),\n",
    "    (1, \"El día no ha tenido sorpresas.\"),\n",
    "    (1, \"Todo está en su sitio como siempre.\"),\n",
    "    (1, \"No me siento diferente a otros días.\"),\n",
    "    (1, \"Es un día más en la semana.\"),\n",
    "    (1, \"Hoy no me siento ni bien ni mal.\"),\n",
    "    (1, \"Mi día ha sido común.\"),\n",
    "    (1, \"No tengo nada que destacar.\"),\n",
    "    (1, \"Hoy todo parece normal.\"),\n",
    "    (1, \"Es otro día como cualquier otro.\"),\n",
    "    (1, \"No hay cambios en mi rutina.\"),\n",
    "    (1, \"El día ha pasado sin incidentes.\"),\n",
    "    (1, \"Todo va como siempre.\"),\n",
    "    (1, \"No me siento particularmente diferente.\"),\n",
    "    (1, \"Nada fuera de lo normal hoy.\"),\n",
    "    (1, \"Es un día completamente regular.\"),\n",
    "    (1, \"Hoy no hay grandes emociones.\"),\n",
    "    (1, \"Es un día en el que nada cambia.\"),\n",
    "    (1, \"Todo está igual que siempre.\"),\n",
    "    (1, \"No hay nada que llame la atención.\"),\n",
    "    (1, \"Hoy es solo otro día.\"),\n",
    "    (1, \"Es un día bastante neutro.\"),\n",
    "    (1, \"Mi ánimo está en su punto medio.\"),\n",
    "    (1, \"Nada ha roto la rutina hoy.\"),\n",
    "    (1, \"No hay nada fuera de lo ordinario.\"),\n",
    "    (1, \"El día se mantiene sin novedades.\"),\n",
    "    (1, \"Hoy me siento indiferente.\"),\n",
    "    (1, \"Es un día sin nada que resaltar.\"),\n",
    "    (1, \"No tengo ninguna emoción fuerte.\"),\n",
    "    (1, \"Todo está en su lugar como siempre.\"),\n",
    "    (1, \"El día sigue su curso habitual.\"),\n",
    "    (1, \"Nada especial ha ocurrido.\"),\n",
    "    (1, \"Hoy todo sigue su curso.\"),\n",
    "    (1, \"Es un día de paz y normalidad.\"),\n",
    "    (1, \"Todo está en calma absoluta.\"),\n",
    "    (1, \"Hoy es un día rutinario.\"),\n",
    "    (1, \"No espero cambios importantes.\"),\n",
    "    (1, \"Me siento en un estado neutral.\"),\n",
    "    (1, \"Hoy todo está en armonía.\"),\n",
    "    (1, \"Es un día de serenidad.\"),\n",
    "    (1, \"No hay nada que me inquiete.\"),\n",
    "    (1, \"Hoy es un día sin grandes sucesos.\"),\n",
    "    (1, \"El día se mantiene tranquilo.\"),\n",
    "    (1, \"No tengo nada emocionante hoy.\"),\n",
    "    (1, \"El día sigue de forma estable.\"),\n",
    "    (1, \"Me siento en paz.\"),\n",
    "    (1, \"Hoy ha sido un día completamente tranquilo.\")\n",
    "])\n",
    "\n",
    "# Textos de \"Triste\"\n",
    "dataset.extend([\n",
    "    (2, \"Me siento muy triste hoy.\"),\n",
    "    (2, \"No tengo ánimos para nada.\"),\n",
    "    (2, \"Siento una profunda tristeza.\"),\n",
    "    (2, \"Nada parece mejorar.\"),\n",
    "    (2, \"Me siento vacío.\"),\n",
    "    (2, \"La vida parece tan difícil.\"),\n",
    "    (2, \"No encuentro consuelo.\"),\n",
    "    (2, \"No puedo dejar de sentirme mal.\"),\n",
    "    (2, \"Todo parece oscuro.\"),\n",
    "    (2, \"No tengo fuerzas para seguir.\"),\n",
    "    (2, \"Siento que todo está perdido.\"),\n",
    "    (2, \"La tristeza me consume.\"),\n",
    "    (2, \"Me siento solo.\"),\n",
    "    (2, \"No encuentro sentido a nada.\"),\n",
    "    (2, \"Me duele el corazón.\"),\n",
    "    (2, \"No veo salida a esta tristeza.\"),\n",
    "    (2, \"Siento que no puedo más.\"),\n",
    "    (2, \"Me siento derrotado.\"),\n",
    "    (2, \"No tengo esperanza.\"),\n",
    "    (2, \"Todo parece sin sentido.\"),\n",
    "    (2, \"No puedo dejar de llorar.\"),\n",
    "    (2, \"Me siento muy desanimado.\"),\n",
    "    (2, \"No encuentro razones para ser feliz.\"),\n",
    "    (2, \"La tristeza me abruma.\"),\n",
    "    (2, \"Me siento completamente solo.\"),\n",
    "    (2, \"No tengo ganas de hacer nada.\"),\n",
    "    (2, \"Estoy en mi peor momento.\"),\n",
    "    (2, \"Me siento roto por dentro.\"),\n",
    "    (2, \"No encuentro nada que me anime.\"),\n",
    "    (2, \"Hoy me siento como si todo estuviera mal.\"),\n",
    "    (2, \"No hay luz en mi vida en este momento.\"),\n",
    "    (2, \"Siento un vacío en mi interior.\"),\n",
    "    (2, \"Nada me hace feliz últimamente.\"),\n",
    "    (2, \"La soledad me invade cada día más.\"),\n",
    "    (2, \"Me siento desconsolado.\"),\n",
    "    (2, \"Es como si estuviera en un pozo sin salida.\"),\n",
    "    (2, \"Todo se ve gris y oscuro.\"),\n",
    "    (2, \"No encuentro paz en mi mente.\"),\n",
    "    (2, \"Es difícil continuar con esta tristeza.\"),\n",
    "    (2, \"Me siento completamente abrumado.\"),\n",
    "    (2, \"El dolor emocional es constante.\"),\n",
    "    (2, \"No hay consuelo que me alivie.\"),\n",
    "    (2, \"Todo en la vida parece vacío.\"),\n",
    "    (2, \"Estoy cansado de sentirme así.\"),\n",
    "    (2, \"La tristeza es mi única compañera.\"),\n",
    "    (2, \"Me siento sin rumbo y perdido.\"),\n",
    "    (2, \"Es difícil ver un futuro positivo.\"),\n",
    "    (2, \"Cada día es una lucha constante.\"),\n",
    "    (2, \"Siento que todo en la vida es inútil.\"),\n",
    "    (2, \"Nada parece tener sentido.\"),\n",
    "    (2, \"No puedo sacarme esta tristeza de encima.\"),\n",
    "    (2, \"Siento que ya no puedo seguir.\"),\n",
    "    (2, \"Cada día parece peor que el anterior.\"),\n",
    "    (2, \"Estoy cansado de sentirme solo.\"),\n",
    "    (2, \"No tengo energías para nada.\"),\n",
    "    (2, \"Cada día es más difícil seguir adelante.\"),\n",
    "    (2, \"Me siento incomprendido y aislado.\"),\n",
    "    (2, \"No veo la luz al final del túnel.\"),\n",
    "    (2, \"Es como si cargara un gran peso en mi alma.\"),\n",
    "    (2, \"No encuentro razones para levantarme.\"),\n",
    "    (2, \"La vida ha perdido su color.\"),\n",
    "    (2, \"Siento que me estoy hundiendo.\"),\n",
    "    (2, \"No tengo la fuerza para luchar.\"),\n",
    "    (2, \"Cada día es un desafío emocional.\"),\n",
    "    (2, \"Siento que nada puede hacerme feliz.\"),\n",
    "    (2, \"La tristeza se ha convertido en parte de mí.\"),\n",
    "    (2, \"No tengo energía para hacer nada.\"),\n",
    "    (2, \"Me siento atrapado en este estado.\"),\n",
    "    (2, \"La soledad es abrumadora.\"),\n",
    "    (2, \"No encuentro paz en mi corazón.\"),\n",
    "    (2, \"El desánimo me invade cada día más.\"),\n",
    "    (2, \"No puedo ver un camino de salida.\"),\n",
    "    (2, \"Cada día es igual de triste.\"),\n",
    "    (2, \"Siento que nada me importa.\"),\n",
    "    (2, \"Me siento apagado por dentro.\"),\n",
    "    (2, \"La tristeza es profunda y constante.\"),\n",
    "    (2, \"Me siento sin fuerzas para seguir adelante.\"),\n",
    "    (2, \"Nada parece tener valor.\"),\n",
    "    (2, \"Es difícil explicar este vacío.\"),\n",
    "    (2, \"Cada día parece más oscuro.\"),\n",
    "    (2, \"No encuentro alegría en nada.\"),\n",
    "    (2, \"La tristeza es parte de mi rutina.\"),\n",
    "    (2, \"No hay motivación en mi vida.\"),\n",
    "    (2, \"Siento que estoy en un túnel sin fin.\"),\n",
    "    (2, \"Cada pensamiento me lleva a la tristeza.\"),\n",
    "    (2, \"Nada me da felicidad.\"),\n",
    "    (2, \"No puedo alejarme de esta oscuridad.\"),\n",
    "    (2, \"Siento que no tengo propósito.\"),\n",
    "    (2, \"La tristeza es todo lo que siento.\"),\n",
    "    (2, \"Me cuesta encontrar algo positivo.\"),\n",
    "    (2, \"Todo lo veo de forma negativa.\"),\n",
    "    (2, \"Siento que no puedo escapar de esto.\"),\n",
    "    (2, \"El dolor es una constante en mi vida.\"),\n",
    "    (2, \"No puedo superar este sentimiento.\"),\n",
    "    (2, \"Cada momento es una carga emocional.\"),\n",
    "    (2, \"Siento que no encajo en ningún lado.\"),\n",
    "    (2, \"La tristeza está arraigada en mí.\"),\n",
    "    (2, \"Cada día parece igual de sombrío.\"),\n",
    "    (2, \"No veo salida a mi situación actual.\")\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S0vylmoTY-6B"
   },
   "source": [
    "## Opción 1: TF-IDF\n",
    "* Vectorizador: TF-IDF\n",
    "* Clasificador: Regresión Logística"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 611,
     "status": "ok",
     "timestamp": 1730333335555,
     "user": {
      "displayName": "Marcela Flaibani",
      "userId": "06388299677795656548"
     },
     "user_tz": 180
    },
    "id": "bCmgg9CWj0qg",
    "outputId": "4f01e242-2728-4386-d474-45e352ad7345"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precisión Regresión Logística Train: 0.9796747967479674\n",
      "Reporte de clasificación Regresión Logística Train:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.98      0.97        84\n",
      "           1       0.98      0.99      0.98        81\n",
      "           2       1.00      0.98      0.99        81\n",
      "\n",
      "    accuracy                           0.98       246\n",
      "   macro avg       0.98      0.98      0.98       246\n",
      "weighted avg       0.98      0.98      0.98       246\n",
      "\n",
      "Precisión Regresión Logística: 0.7903225806451613\n",
      "Reporte de clasificación Regresión Logística:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.58      0.72        24\n",
      "           1       0.69      0.90      0.78        20\n",
      "           2       0.81      0.94      0.87        18\n",
      "\n",
      "    accuracy                           0.79        62\n",
      "   macro avg       0.81      0.81      0.79        62\n",
      "weighted avg       0.82      0.79      0.78        62\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\flaib\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "c:\\Users\\flaib\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Obtenemos las stopwords para español\n",
    "spanish_stop_words = stopwords.words('spanish')\n",
    "\n",
    "# Lista de funciones que deseas aplicar\n",
    "preprocess_functions = [to_lower, eliminate_punctuation, eliminate_accents]\n",
    "\n",
    "# Preprocesar el dataset\n",
    "# Aplica el pipeline solo con las funciones seleccionadas\n",
    "X = [pipeline(text, preprocess_functions) for _, text in dataset]\n",
    "y = [label for label, _ in dataset]\n",
    "\n",
    "# División del dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Vectorización sin stop_words\n",
    "vectorizer = TfidfVectorizer(stop_words=spanish_stop_words)\n",
    "X_train_vectorized = vectorizer.fit_transform(X_train)\n",
    "X_test_vectorized = vectorizer.transform(X_test)\n",
    "\n",
    "# Creación y entrenamiento del modelo de Regresión Logística con multinomial\n",
    "model_LR = LogisticRegression(max_iter=1000, multi_class='multinomial', solver='lbfgs')\n",
    "model_LR.fit(X_train_vectorized, y_train)\n",
    "\n",
    "# Evaluación del modelo de Regresión Logística Train\n",
    "y_pred_LR_train = model_LR.predict(X_train_vectorized)\n",
    "acc_LR_train = accuracy_score(y_train, y_pred_LR_train)\n",
    "report_LR_train = classification_report(y_train, y_pred_LR_train, zero_division=1)\n",
    "print(\"Precisión Regresión Logística Train:\", acc_LR_train)\n",
    "print(\"Reporte de clasificación Regresión Logística Train:\\n\", report_LR_train)\n",
    "\n",
    "# Evaluación del modelo de Regresión Logística Test\n",
    "y_pred_LR = model_LR.predict(X_test_vectorized)\n",
    "acc_LR = accuracy_score(y_test, y_pred_LR)\n",
    "report_LR = classification_report(y_test, y_pred_LR, zero_division=1)\n",
    "print(\"Precisión Regresión Logística:\", acc_LR)\n",
    "print(\"Reporte de clasificación Regresión Logística:\\n\", report_LR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bq7S0C67Uyn3"
   },
   "source": [
    "### prueba para varias frases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1730333335555,
     "user": {
      "displayName": "Marcela Flaibani",
      "userId": "06388299677795656548"
     },
     "user_tz": 180
    },
    "id": "NLRcDhothXKd",
    "outputId": "e12dfbae-a7d5-4ba7-c973-401288c28f0b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La frase 'no puedo dejar de sonreir' pertenece a la categoría: Alegre\n",
      "La frase 'hoy es un dia promedio' pertenece a la categoría: Neutral\n",
      "La frase 'me siento mal' pertenece a la categoría: Triste\n",
      "La frase 'que triste estoy' pertenece a la categoría: Triste\n",
      "La frase 'solo quiero estar solo y reflexionar' pertenece a la categoría: Triste\n"
     ]
    }
   ],
   "source": [
    "# Definimos una lista de frases para clasificar\n",
    "new_phrases = [\n",
    "    \"No puedo dejar de sonreír\",\n",
    "    \"Hoy es un día promedio\",\n",
    "    \"me siento mal\",\n",
    "    \"Qué triste estoy\",\n",
    "    \"solo quiero estar solo y reflexionar\",\n",
    "]\n",
    "\n",
    "# Convertimos las frases a minúsculas\n",
    "new_phrases = [pipeline(text, preprocess_functions) for text in new_phrases]\n",
    "\n",
    "# Transformamos las nuevas frases usando el vectorizador que usamos para entrenar el modelo\n",
    "new_phrases_vectorized = vectorizer.transform(new_phrases)\n",
    "\n",
    "# Usamos el modelo entrenado para predecir las etiquetas de las nuevas frases\n",
    "new_predictions = model_LR.predict(new_phrases_vectorized)\n",
    "\n",
    "# Imprimimos las etiquetas predichas\n",
    "for i, label in enumerate(new_predictions):\n",
    "    print(f\"La frase '{new_phrases[i]}' pertenece a la categoría: {labels[label][1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C78361jjUvJu"
   },
   "source": [
    "### para una sola frase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1730333335555,
     "user": {
      "displayName": "Marcela Flaibani",
      "userId": "06388299677795656548"
     },
     "user_tz": 180
    },
    "id": "hgoVkuJkUtQ3",
    "outputId": "9fe33759-8c1a-41e4-b3b5-92ed54c7f930"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La frase 'no puedo dejar de sonreir' pertenece a la categoría: Alegre\n"
     ]
    }
   ],
   "source": [
    "# Diccionario de etiquetas\n",
    "labels_dicc = {0: \"Alegre\", 1: \"Neutral\", 2: \"Triste\"}\n",
    "\n",
    "# Nueva frase para clasificar\n",
    "new_phrase = \"No puedo dejar de sonreír\"\n",
    "\n",
    "# Aplicamos el preprocesamiento a la frase\n",
    "new_phrase = pipeline(new_phrase, preprocess_functions)\n",
    "\n",
    "# Transformamos la frase usando el vectorizador con el que entrenamos el modelo\n",
    "new_phrase_vectorized = vectorizer.transform([new_phrase])\n",
    "\n",
    "# Usamos el modelo entrenado para predecir la etiqueta de la frase\n",
    "new_prediction = model_LR.predict(new_phrase_vectorized)[0]\n",
    "\n",
    "# Imprimimos la etiqueta predicha\n",
    "print(f\"La frase '{new_phrase}' pertenece a la categoría: {labels_dicc[new_prediction]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z1orpBHKYx7I"
   },
   "source": [
    "## Opción 2: SentenceTransformer (Elegida - mejores métricas)\n",
    "* Vectorizador: semántico/modelo de embeding all-mpnet-base-v2\n",
    "* Clasificador: Regresión Logística"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OJimKMnOBRG6"
   },
   "source": [
    "Se saca el preprocesamiento de los datos en el modelo de embedding porque no es necesario y mejoran las métricas.\n",
    "\n",
    "\n",
    "preprocess_functions_Em = [to_lower, eliminate_accents]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenar el modelo de regresión Logística (una sola vez para guardar en el Drive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7990,
     "status": "ok",
     "timestamp": 1730334043361,
     "user": {
      "displayName": "Marcela Flaibani",
      "userId": "06388299677795656548"
     },
     "user_tz": 180
    },
    "id": "0q6GQbVdYySe",
    "outputId": "4b5b27b7-08f3-42c4-a1c0-6a434405999b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precisión Regresión Logística Train: 0.8699186991869918\n",
      "Reporte de clasificación Regresión Logística Train:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.83      0.86        84\n",
      "           1       0.87      0.85      0.86        81\n",
      "           2       0.85      0.93      0.89        81\n",
      "\n",
      "    accuracy                           0.87       246\n",
      "   macro avg       0.87      0.87      0.87       246\n",
      "weighted avg       0.87      0.87      0.87       246\n",
      "\n",
      "Precisión Regresión Logística Test: 0.8870967741935484\n",
      "Reporte de clasificación Regresión Logística Test:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.83      0.89        24\n",
      "           1       0.82      0.90      0.86        20\n",
      "           2       0.89      0.94      0.92        18\n",
      "\n",
      "    accuracy                           0.89        62\n",
      "   macro avg       0.89      0.89      0.89        62\n",
      "weighted avg       0.89      0.89      0.89        62\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\flaib\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\flaib\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# Cargamos el modelo desde HuggingFace https://huggingface.co/sentence-transformers/all-mpnet-base-v2\n",
    "# model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
    "# model = SentenceTransformer('msmarco-MiniLM-L-6-v3')\n",
    "model = SentenceTransformer('intfloat/multilingual-e5-small')\n",
    "\n",
    "# Preparar X e y\n",
    "# Aplica el pipeline solo con las funciones seleccionadas\n",
    "X = [text for label, text in dataset]\n",
    "y = [label for label, text in dataset]\n",
    "\n",
    "# División del dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Obtenemos los embeddings de BERT para los conjuntos de entrenamiento y prueba\n",
    "X_train_vectorized = model.encode(X_train)\n",
    "X_test_vectorized = model.encode(X_test)\n",
    "\n",
    "# Creación y entrenamiento del modelo de Regresión Logística Multinomial\n",
    "model_LR_Em = LogisticRegression(max_iter=10, multi_class='multinomial', solver='lbfgs')\n",
    "model_LR_Em.fit(X_train_vectorized, y_train)\n",
    "\n",
    "# Evaluación del modelo de Regresión Logística en Train\n",
    "y_pred_LR_Em_train = model_LR_Em.predict(X_train_vectorized)\n",
    "acc_LR_Em_train = accuracy_score(y_train, y_pred_LR_Em_train)\n",
    "report_LR_Em_train = classification_report(y_train, y_pred_LR_Em_train, zero_division=1)\n",
    "print(\"Precisión Regresión Logística Train:\", acc_LR_Em_train)\n",
    "print(\"Reporte de clasificación Regresión Logística Train:\\n\", report_LR_Em_train)\n",
    "\n",
    "# Evaluación del modelo de Regresión Logística en Test\n",
    "y_pred_LR_Em = model_LR_Em.predict(X_test_vectorized)\n",
    "acc_LR_Em = accuracy_score(y_test, y_pred_LR_Em)\n",
    "report_LR_Em = classification_report(y_test, y_pred_LR_Em, zero_division=1)\n",
    "print(\"Precisión Regresión Logística Test:\", acc_LR_Em)\n",
    "print(\"Reporte de clasificación Regresión Logística Test:\\n\", report_LR_Em)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guardar el modelo de Regresión Logística entrenado en el Drive (comentado para la entrega del TP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 23807,
     "status": "ok",
     "timestamp": 1730334311009,
     "user": {
      "displayName": "Marcela Flaibani",
      "userId": "06388299677795656548"
     },
     "user_tz": 180
    },
    "id": "HBLv5w6ozT3U",
    "outputId": "38597e49-3ffb-4749-98ed-c932f7fc5413"
   },
   "outputs": [],
   "source": [
    "# # Montar Google Drive para acceder a los embeddings guardados\n",
    "# drive.mount('/content/drive')\n",
    "# drive_path = '/content/drive/My Drive/Colab Notebooks/NLP/TP_1/'\n",
    "\n",
    "# # Ruta de Google Drive\n",
    "# model_path = drive_path + 'modelo_LR_Em.joblib'\n",
    "\n",
    "# # Guardar el modelo en la ruta especificada\n",
    "# dump(model_LR_Em, model_path)\n",
    "# print(f\"Modelo guardado en: {model_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cargar el modelo de Regresión Logística entrenado desde el Drive (comentado para la entrega del TP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 301,
     "status": "ok",
     "timestamp": 1730334326298,
     "user": {
      "displayName": "Marcela Flaibani",
      "userId": "06388299677795656548"
     },
     "user_tz": 180
    },
    "id": "W-FgJ1bRzr0V",
    "outputId": "6eee3cf5-7327-42cf-87c8-e88be208ff6e"
   },
   "outputs": [],
   "source": [
    "# # Cargar el modelo desde la ruta de Google Drive\n",
    "# model_LR_Em = load(model_path)\n",
    "# print(\"Modelo cargado con éxito.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cargar el modelo de Regresión Logística entrenado desde GitHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_github(url):\n",
    "    # Descarga el archivo desde GitHub\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()  # Verifica que la descarga fue exitosa\n",
    "    # Carga el contenido con joblib\n",
    "    modelo = joblib.load(BytesIO(response.content))\n",
    "    return modelo\n",
    "\n",
    "# URL del archivo en GitHub\n",
    "url = \"https://github.com/flaibani/NLP/raw/main/TP_1/Modelos/modelo_LR_Em.joblib\"\n",
    "# Cargar el modelo de Regresión logística\n",
    "model_LR_Em= load_model_github(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3H3y2raDliKt"
   },
   "source": [
    "### Prueba para varias frases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 599,
     "status": "ok",
     "timestamp": 1730334331989,
     "user": {
      "displayName": "Marcela Flaibani",
      "userId": "06388299677795656548"
     },
     "user_tz": 180
    },
    "id": "ZSEpmhyKdU2m",
    "outputId": "46fe5f5f-5f87-4959-8f3d-f01a249fd65c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texto: 'Me gusta sonreir'\n",
      "Clasificación predicha: Alegre\n",
      "\n",
      "Texto: 'Hoy es un día promedio'\n",
      "Clasificación predicha: Neutral\n",
      "\n",
      "Texto: 'me siento mal'\n",
      "Clasificación predicha: Triste\n",
      "\n",
      "Texto: 'Qué triste estoy'\n",
      "Clasificación predicha: Triste\n",
      "\n",
      "Texto: 'solo quiero estar solo y reflexionar'\n",
      "Clasificación predicha: Triste\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Nuevas frases para clasificar\n",
    "new_phrases = [\n",
    "    \"Me gusta sonreir\",\n",
    "    \"Hoy es un día promedio\",\n",
    "    \"me siento mal\",\n",
    "    \"Qué triste estoy\",\n",
    "    \"solo quiero estar solo y reflexionar\",\n",
    "]\n",
    "\n",
    "# Vectorización de las nuevas frases  usando el vectorizador con el que entrenamos el modelo\n",
    "new_phrases_vectorized = model.encode(new_phrases)\n",
    "\n",
    "# Haciendo predicciones con el modelo entrenado\n",
    "new_predictions = model_LR_Em.predict(new_phrases_vectorized)\n",
    "\n",
    "# Mostrando las predicciones junto con las frases\n",
    "for text, label in zip(new_phrases, new_predictions):\n",
    "    print(f\"Texto: '{text}'\")\n",
    "    print(f\"Clasificación predicha: {labels[label][1]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a4_IKGeclmKk"
   },
   "source": [
    "### Prueba para una sola frase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 302,
     "status": "ok",
     "timestamp": 1730334339277,
     "user": {
      "displayName": "Marcela Flaibani",
      "userId": "06388299677795656548"
     },
     "user_tz": 180
    },
    "id": "ylDm2lkzmClO",
    "outputId": "1f4d66e9-0ea2-4bd8-ab48-dfea3fa755b8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La frase 'No puedo dejar de reír' pertenece a la categoría: Alegre\n"
     ]
    }
   ],
   "source": [
    "# Diccionario de etiquetas\n",
    "labels_dicc = {0: \"Alegre\", 1: \"Neutral\", 2: \"Triste\"}\n",
    "\n",
    "# Nueva frase para clasificar\n",
    "new_phrase = \"No puedo dejar de reír\"\n",
    "\n",
    "# Vectorización de la nueva frase usando el vectorizador con el que entrenamos el modelo\n",
    "new_phrase_vectorized = model.encode([new_phrase])\n",
    "\n",
    "# Usamos el modelo entrenado para predecir la etiqueta de la frase\n",
    "new_prediction = model_LR_Em.predict(new_phrase_vectorized)[0]\n",
    "\n",
    "# Imprimimos la etiqueta predicha\n",
    "print(f\"La frase '{new_phrase}' pertenece a la categoría: {labels_dicc[new_prediction]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e0zKKEfQ2arq"
   },
   "source": [
    "## Opción 3 (No se puede ejecutar desde GitHub)\n",
    "\n",
    "No se pudo cargar el archivo \"training.1600000.processed.noemoticon.csv\" en GitHub por su tamaño.\n",
    "\n",
    "* Descargar Sentiment140 dataset with 1.6 million tweets (https://www.kaggle.com/datasets/kazanova/sentiment140?resource=download) en inglés\n",
    "* Guardar en un dataset\n",
    "* Entrenar el modelo de regresión\n",
    "* Traducir al inglés la frase del usuario\n",
    "* Predecir etiqueta (Positivo o Negativo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "aborted",
     "timestamp": 1730333367102,
     "user": {
      "displayName": "Marcela Flaibani",
      "userId": "06388299677795656548"
     },
     "user_tz": 180
    },
    "id": "YWo17RL0Antx"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\"gdown\" no se reconoce como un comando interno o externo,\n",
      "programa o archivo por lotes ejecutable.\n"
     ]
    }
   ],
   "source": [
    "!gdown 1KRJSZXwNdreLshCG17FkPZF7cmpR5KBc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "aborted",
     "timestamp": 1730333367102,
     "user": {
      "displayName": "Marcela Flaibani",
      "userId": "06388299677795656548"
     },
     "user_tz": 180
    },
    "id": "HBxSaPcj_c8b"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/content/training.1600000.processed.noemoticon.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [16], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m ruta_archivo \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/content/training.1600000.processed.noemoticon.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Carga el CSV, especificando que no tiene cabecera (para evitar errores de nombre de columnas)\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mruta_archivo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mISO-8859-1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Renombra las columnas para un acceso más fácil\u001b[39;00m\n\u001b[0;32m     11\u001b[0m df\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msentimiento\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfecha\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124musuario\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtexto\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\flaib\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\flaib\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\flaib\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\flaib\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\Users\\flaib\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/training.1600000.processed.noemoticon.csv'"
     ]
    }
   ],
   "source": [
    "labels = [(0, \"negativo\"), (4, \"positivo\")]#, (2, \"Neutral\")]\n",
    "dataset = []\n",
    "\n",
    "# Carga el archivo CSV (ajusta la ruta al archivo de Sentiment140)\n",
    "ruta_archivo = \"/content/training.1600000.processed.noemoticon.csv\"\n",
    "\n",
    "# Carga el CSV, especificando que no tiene cabecera (para evitar errores de nombre de columnas)\n",
    "df = pd.read_csv(ruta_archivo, encoding=\"ISO-8859-1\", header=None)\n",
    "\n",
    "# Renombra las columnas para un acceso más fácil\n",
    "df.columns = [\"sentimiento\", \"id\", \"fecha\", \"query\", \"usuario\", \"texto\"]\n",
    "\n",
    "# Selecciona solo las columnas de interés\n",
    "df = df[[\"sentimiento\", \"texto\"]]\n",
    "\n",
    "# Muestra algunas filas para verificar\n",
    "print(df.head())\n",
    "\n",
    "# Convertir el DataFrame en una lista de tuplas (sentimiento, texto)\n",
    "dataset = [(fila.sentimiento, fila.texto) for fila in df.itertuples(index=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "aborted",
     "timestamp": 1730333367102,
     "user": {
      "displayName": "Marcela Flaibani",
      "userId": "06388299677795656548"
     },
     "user_tz": 180
    },
    "id": "LuDpUkf1CHGW"
   },
   "outputs": [],
   "source": [
    "# Descargamos los stopwords que necesitaremos luego\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Obtenemos las stopwords para español\n",
    "spanish_stop_words = stopwords.words('english')\n",
    "\n",
    "# Preparar X e y\n",
    "X = [text.lower() for label, text in dataset]\n",
    "y = [label for label, text in dataset]\n",
    "\n",
    "# División del dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Vectorización de los textos con eliminación de palabras vacías\n",
    "vectorizer = TfidfVectorizer(stop_words=spanish_stop_words)\n",
    "X_train_vectorized = vectorizer.fit_transform(X_train)\n",
    "X_test_vectorized = vectorizer.transform(X_test)\n",
    "\n",
    "# Creación y entrenamiento del modelo de Regresión Logística con multinomial\n",
    "modelo_LR = LogisticRegression(max_iter=1000, multi_class='multinomial', solver='lbfgs')\n",
    "modelo_LR.fit(X_train_vectorized, y_train)\n",
    "\n",
    "# Evaluación del modelo de Regresión Logística\n",
    "y_pred_LR = modelo_LR.predict(X_test_vectorized)\n",
    "acc_LR = accuracy_score(y_test, y_pred_LR)\n",
    "report_LR = classification_report(y_test, y_pred_LR, zero_division=1)\n",
    "\n",
    "print(\"Precisión Regresión Logística:\", acc_LR)\n",
    "print(\"Reporte de clasificación Regresión Logística:\\n\", report_LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "aborted",
     "timestamp": 1730333367102,
     "user": {
      "displayName": "Marcela Flaibani",
      "userId": "06388299677795656548"
     },
     "user_tz": 180
    },
    "id": "Hdu01Vza2dzc"
   },
   "outputs": [],
   "source": [
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "# Define el modelo y el tokenizador\n",
    "modelo = 'Helsinki-NLP/opus-mt-es-en'\n",
    "tokenizer = MarianTokenizer.from_pretrained(modelo)\n",
    "model = MarianMTModel.from_pretrained(modelo)\n",
    "\n",
    "# Define el texto en español que quieres traducir al inglés\n",
    "texto_español = \"No puedo dejar de sonreir.\"\n",
    "\n",
    "# Tokeniza el texto y genera la traducción\n",
    "inputs = tokenizer(texto_español, return_tensors=\"pt\")\n",
    "outputs = model.generate(**inputs)\n",
    "# Decodifica y muestra la traducción\n",
    "texto_ingles = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(texto_ingles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "aborted",
     "timestamp": 1730333367102,
     "user": {
      "displayName": "Marcela Flaibani",
      "userId": "06388299677795656548"
     },
     "user_tz": 180
    },
    "id": "fPXPYucFKkeu"
   },
   "outputs": [],
   "source": [
    "labels_dict = {0: \"negativo\", 4: \"positivo\"}\n",
    "\n",
    "# Definir y transformar la frase para clasificación\n",
    "frase_clasificar = texto_ingles.lower()\n",
    "frase_vectorizada = vectorizer.transform([frase_clasificar])\n",
    "\n",
    "# Predecir la etiqueta de la frase\n",
    "etiqueta_predicha = modelo_LR.predict(frase_vectorizada)[0]\n",
    "\n",
    "# Imprimir el resultado\n",
    "print(f\"La frase '{frase_clasificar}' pertenece a la categoría: {labels_dict[etiqueta_predicha]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NlpzVzjorG9D"
   },
   "source": [
    "## Opcion 4: KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "aborted",
     "timestamp": 1730333367103,
     "user": {
      "displayName": "Marcela Flaibani",
      "userId": "06388299677795656548"
     },
     "user_tz": 180
    },
    "id": "lSdDK1I-rFjC"
   },
   "outputs": [],
   "source": [
    "# Obtenemos las stopwords para español\n",
    "spanish_stop_words = stopwords.words('spanish')\n",
    "\n",
    "# Lista de funciones que deseas aplicar (suponiendo que están definidas)\n",
    "preprocess_functions = [to_lower, eliminate_punctuation, eliminate_accents]\n",
    "\n",
    "# Preprocesar el dataset\n",
    "X = [pipeline(text, preprocess_functions) for _, text in dataset]\n",
    "y = [label for label, _ in dataset]\n",
    "\n",
    "# División del dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Vectorización sin stop_words (ya se eliminaron en el preprocesamiento)\n",
    "vectorizer = TfidfVectorizer(stop_words=spanish_stop_words)\n",
    "X_train_vectorized = vectorizer.fit_transform(X_train)\n",
    "X_test_vectorized = vectorizer.transform(X_test)\n",
    "\n",
    "# Creación y entrenamiento del modelo KNN\n",
    "k = 4  # número de vecinos\n",
    "model_KNN = KNeighborsClassifier(n_neighbors=k)\n",
    "model_KNN.fit(X_train_vectorized, y_train)\n",
    "\n",
    "# Evaluación del modelo KNN\n",
    "y_pred_KNN = model_KNN.predict(X_test_vectorized)\n",
    "acc_KNN = accuracy_score(y_test, y_pred_KNN)\n",
    "report_KNN = classification_report(y_test, y_pred_KNN, zero_division=1)\n",
    "\n",
    "print(\"Precisión K-Nearest Neighbors:\", acc_KNN)\n",
    "print(\"Reporte de clasificación K-Nearest Neighbors:\\n\", report_KNN)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KAEPZZzoUz_9"
   },
   "source": [
    "# Cargar los datasets de Películas, Juegos y Libros.\n",
    "\n",
    "* IMDB-Movie-Data.csv\n",
    "* bgg_database.csv\n",
    "* gutenberg_books_detailed.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "executionInfo": {
     "elapsed": 798,
     "status": "ok",
     "timestamp": 1730334386873,
     "user": {
      "displayName": "Marcela Flaibani",
      "userId": "06388299677795656548"
     },
     "user_tz": 180
    },
    "id": "99y3-5nFU89H"
   },
   "outputs": [],
   "source": [
    "df_movies = pd.read_csv('https://github.com/flaibani/NLP/raw/main/TP_1/Archivos_CSV/IMDB-Movie-Data.csv')\n",
    "df_games = pd.read_csv('https://github.com/flaibani/NLP/raw/main/TP_1/Archivos_CSV/bgg_database.csv')\n",
    "df_books = pd.read_csv('https://github.com/flaibani/NLP/raw/main/TP_1/Archivos_CSV/gutenberg_books_detailed.csv')\n",
    "\n",
    "# # Definir las columnas relevantes y llenar los NaN con texto vacío\n",
    "# movies_desc = df_movies['Description'].fillna('')\n",
    "# games_desc = df_games['description'].fillna('')\n",
    "# books_desc = df_books['Summary'].fillna('')\n",
    "\n",
    "# Combinar columnas de interés y llenar los NaN con texto vacío\n",
    "df_movies['full_description'] = (\n",
    "    df_movies['Genre'].fillna('') + ' ' +\n",
    "    df_movies['Description'].fillna('') + ' ' +\n",
    "    df_movies['Director'].fillna('') + ' ' +\n",
    "    df_movies['Actors'].fillna('')\n",
    ")\n",
    "\n",
    "df_games['full_description'] = (\n",
    "    df_games['game_name'].fillna('') + ' ' +\n",
    "    df_games['description'].fillna('') + ' ' +\n",
    "    df_games['categories'].fillna('')\n",
    ")\n",
    "\n",
    "df_books['full_description'] = (\n",
    "    df_books['Title'].fillna('') + ' ' +\n",
    "    df_books['Author'].fillna('') + ' ' +\n",
    "    df_books['Summary'].fillna('') + ' ' +\n",
    "    df_books['Subjects'].fillna('')\n",
    ")\n",
    "\n",
    "movies_desc = df_movies['full_description']\n",
    "games_desc = df_games['full_description']\n",
    "books_desc = df_books['full_description']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "THc1oZp0m5lq"
   },
   "source": [
    "# Preferencias del Usuario"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4FdpOdKQpC_P"
   },
   "source": [
    "## Traducir de español a inglés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "executionInfo": {
     "elapsed": 310,
     "status": "ok",
     "timestamp": 1730336487846,
     "user": {
      "displayName": "Marcela Flaibani",
      "userId": "06388299677795656548"
     },
     "user_tz": 180
    },
    "id": "87xgjjDhosRY"
   },
   "outputs": [],
   "source": [
    "mood = new_prediction\n",
    "user_preferences_es = \"una historia de amor en la selva\"\n",
    "# user_preferences_es = \"quiero ver algo de Ridley Scott\"\n",
    "user_preferences_es = \"quiero que actue Chris Pratt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4476,
     "status": "ok",
     "timestamp": 1730336495152,
     "user": {
      "displayName": "Marcela Flaibani",
      "userId": "06388299677795656548"
     },
     "user_tz": 180
    },
    "id": "JPU0LzRPo_gI",
    "outputId": "67f311c8-b4e1-4840-b17a-3b8dcba67099"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I want you to act Chris Pratt.\n",
      "i want you to act chris pratt\n"
     ]
    }
   ],
   "source": [
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "# Define el modelo y el tokenizador\n",
    "select_model = 'Helsinki-NLP/opus-mt-es-en'\n",
    "tokenizer = MarianTokenizer.from_pretrained(select_model)\n",
    "model = MarianMTModel.from_pretrained(select_model)\n",
    "\n",
    "# Tokeniza el texto y genera la traducción\n",
    "inputs = tokenizer(user_preferences_es, return_tensors=\"pt\")\n",
    "outputs = model.generate(**inputs)\n",
    "# Decodifica y muestra la traducción\n",
    "user_preferences_en = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(user_preferences_en)\n",
    "user_preferences_en_clean = pipeline(user_preferences_en, preprocess_functions)\n",
    "print(user_preferences_en_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DLXfk2jtVUZO"
   },
   "source": [
    "## Opción 1: TF-IDF\n",
    "Vectorizar los 3 datasets (películas, juegos y libros) por separado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VOeepIgq3B1a"
   },
   "source": [
    "### Preprocesar para TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "executionInfo": {
     "elapsed": 278,
     "status": "ok",
     "timestamp": 1730252148112,
     "user": {
      "displayName": "Marcela Flaibani",
      "userId": "06388299677795656548"
     },
     "user_tz": 180
    },
    "id": "NVmCb_pU2-2D"
   },
   "outputs": [],
   "source": [
    "# Lista de funciones que deseas aplicar\n",
    "preprocess_functions = [to_lower, eliminate_punctuation, eliminate_accents]\n",
    "\n",
    "# Preprocesar el dataset\n",
    "# Aplica el pipeline solo con las funciones seleccionadas\n",
    "movies_desc_clean = [pipeline(text, preprocess_functions) for text in movies_desc]\n",
    "games_desc_clean = [pipeline(text, preprocess_functions) for text in games_desc]\n",
    "books_desc_clean = [pipeline(text, preprocess_functions) for text in books_desc]\n",
    "\n",
    "# Convertir las listas preprocesadas de nuevo a Series\n",
    "movies_desc_clean = pd.Series(movies_desc)\n",
    "games_desc_clean = pd.Series(games_desc)\n",
    "books_desc_clean = pd.Series(books_desc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtener recomendaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 638,
     "status": "ok",
     "timestamp": 1730252182471,
     "user": {
      "displayName": "Marcela Flaibani",
      "userId": "06388299677795656548"
     },
     "user_tz": 180
    },
    "id": "BrUr8Rv-J_rx",
    "outputId": "8a3162c8-88ee-4438-8020-4bbe55ae00da"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Películas recomendadas:\n",
      "- Guardians of the Galaxy (Similitud: 0.17)\n",
      "- The Magnificent Seven (Similitud: 0.16)\n",
      "- 10 Years (Similitud: 0.16)\n",
      "\n",
      "Juegos de mesa recomendados:\n",
      "- Nemo's War (Second Edition) (Similitud: 0.05)\n",
      "- Great Western Trail: Second Edition (Similitud: 0.05)\n",
      "- Container (Similitud: 0.05)\n",
      "\n",
      "Libros recomendados:\n",
      "- Salomé: A Tragedy in One Act (Similitud: 0.06)\n",
      "- The Prince and the Pauper (Similitud: 0.05)\n",
      "- On the Duty of Civil Disobedience (Similitud: 0.05)\n"
     ]
    }
   ],
   "source": [
    "# Función para obtener recomendaciones\n",
    "def get_recommendations(descriptions, user_preference, titles, top_n=3):\n",
    "    # Ajustar el vectorizador a los datos específicos y la preferencia del usuario\n",
    "    vectorizer = TfidfVectorizer(stop_words='english')\n",
    "    combined_data = pd.concat([descriptions, pd.Series([user_preference])], ignore_index=True)\n",
    "\n",
    "    # Vectorizar\n",
    "    tfidf_matrix = vectorizer.fit_transform(combined_data)\n",
    "\n",
    "    # Separar el vector de la preferencia del usuario\n",
    "    user_vector = tfidf_matrix[-1]\n",
    "    tfidf_matrix = tfidf_matrix[:-1]\n",
    "\n",
    "    # Calcular similitudes de coseno\n",
    "    cosine_similarities = cosine_similarity(user_vector, tfidf_matrix).flatten()\n",
    "    similar_indices = cosine_similarities.argsort()[-top_n:][::-1]\n",
    "    recommendations = [(titles.iloc[i], cosine_similarities[i]) for i in similar_indices]\n",
    "    return recommendations\n",
    "\n",
    "# Obtener recomendaciones para cada tipo de contenido\n",
    "movie_recommendations = get_recommendations(movies_desc_clean, user_preferences_en_clean, df_movies['Title'])\n",
    "game_recommendations = get_recommendations(games_desc_clean, user_preferences_en_clean, df_games['game_name'])\n",
    "book_recommendations = get_recommendations(books_desc_clean, user_preferences_en_clean, df_books['Title'])\n",
    "\n",
    "# Mostrar recomendaciones\n",
    "print(\"Películas recomendadas:\")\n",
    "for title, score in movie_recommendations:\n",
    "    print(f\"- {title} (Similitud: {score:.2f})\")\n",
    "\n",
    "print(\"\\nJuegos de mesa recomendados:\")\n",
    "for name, score in game_recommendations:\n",
    "    print(f\"- {name} (Similitud: {score:.2f})\")\n",
    "\n",
    "print(\"\\nLibros recomendados:\")\n",
    "for title, score in book_recommendations:\n",
    "    print(f\"- {title} (Similitud: {score:.2f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KltQD9b6VFqn"
   },
   "source": [
    "## Opción 2: SentenceTransformer (Elegida)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jI6EIJ-pRBbR"
   },
   "source": [
    "### Entrenar los modelos (una sola vez para guardar en el Drive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "executionInfo": {
     "elapsed": 1058761,
     "status": "ok",
     "timestamp": 1730337862332,
     "user": {
      "displayName": "Marcela Flaibani",
      "userId": "06388299677795656548"
     },
     "user_tz": 180
    },
    "id": "yvT5DpUNHsme"
   },
   "outputs": [],
   "source": [
    "# Inicializar el modelo de SentenceTransformer\n",
    "# model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "# model = SentenceTransformer('msmarco-MiniLM-L-6-v3')\n",
    "model = SentenceTransformer('intfloat/multilingual-e5-small') # demora muchísimo/ similitudes arriba del 80%\n",
    "\n",
    "# Calcular los embeddings de las descripciones y almacenarlos\n",
    "movie_embeddings = model.encode(movies_desc.to_list(), convert_to_tensor=True)\n",
    "game_embeddings = model.encode(games_desc.to_list(), convert_to_tensor=True)\n",
    "book_embeddings = model.encode(books_desc.to_list(), convert_to_tensor=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guardar los modelos entrenados en el Drive (comentado para la entrega del TP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "executionInfo": {
     "elapsed": 639,
     "status": "ok",
     "timestamp": 1730337989963,
     "user": {
      "displayName": "Marcela Flaibani",
      "userId": "06388299677795656548"
     },
     "user_tz": 180
    },
    "id": "SV8QcBbgQDWA"
   },
   "outputs": [],
   "source": [
    "# # Define la ruta específica en tu Google Drive\n",
    "# drive_path = '/content/drive/My Drive/Colab Notebooks/NLP/TP_1/'\n",
    "\n",
    "# # Guarda los embeddings como archivos .pt (en formato PyTorch)\n",
    "# torch.save(movie_embeddings, drive_path + 'movie_embeddings.pt')\n",
    "# torch.save(game_embeddings, drive_path + 'game_embeddings.pt')\n",
    "# torch.save(book_embeddings, drive_path + 'book_embeddings.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uMNOhE2bRNik"
   },
   "source": [
    "### Cargar los modelos entrenados desde el Drive (comentado para la entrega del TP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2167,
     "status": "ok",
     "timestamp": 1730338020631,
     "user": {
      "displayName": "Marcela Flaibani",
      "userId": "06388299677795656548"
     },
     "user_tz": 180
    },
    "id": "jOUzaLheRUge",
    "outputId": "7b7cfd2e-4afb-4cf3-cca3-a7ba196c5e3c"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 280,
     "status": "ok",
     "timestamp": 1730338023835,
     "user": {
      "displayName": "Marcela Flaibani",
      "userId": "06388299677795656548"
     },
     "user_tz": 180
    },
    "id": "2ocyekNzQr5t",
    "outputId": "3e57e853-8497-481b-bb93-87c9d789b23c"
   },
   "outputs": [],
   "source": [
    "# # Cargar desde archivos .pt\n",
    "# movie_embeddings = torch.load(drive_path + 'movie_embeddings.pt')\n",
    "# game_embeddings = torch.load(drive_path + 'game_embeddings.pt')\n",
    "# book_embeddings = torch.load(drive_path + 'book_embeddings.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cargar los modelos entrenados desde GitHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\flaib\\AppData\\Local\\Temp\\ipykernel_4752\\2344808435.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(BytesIO(response.content), map_location='cpu')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelos cargados con éxito.\n"
     ]
    }
   ],
   "source": [
    "# Función para descargar y cargar el archivo\n",
    "def download_and_load(url):\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()  # Verifica que la descarga fue exitosa\n",
    "    return torch.load(BytesIO(response.content), map_location='cpu')\n",
    "\n",
    "# URLs crudas de los archivos en GitHub\n",
    "movie_url = 'https://github.com/flaibani/NLP/raw/main/TP_1/Modelos/movie_embeddings.pt'\n",
    "game_url = 'https://github.com/flaibani/NLP/raw/main/TP_1/Modelos/game_embeddings.pt'\n",
    "book_url = 'https://github.com/flaibani/NLP/raw/main/TP_1/Modelos/book_embeddings.pt'\n",
    "\n",
    "# Cargar los embeddings\n",
    "movie_embeddings = download_and_load(movie_url)\n",
    "game_embeddings = download_and_load(game_url)\n",
    "book_embeddings = download_and_load(book_url)\n",
    "print(\"Modelos cargados con éxito.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtener recomendaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "executionInfo": {
     "elapsed": 310,
     "status": "ok",
     "timestamp": 1730338031134,
     "user": {
      "displayName": "Marcela Flaibani",
      "userId": "06388299677795656548"
     },
     "user_tz": 180
    },
    "id": "DThLwLpqH6wx"
   },
   "outputs": [],
   "source": [
    "# user_preferences_en = 'A story of action, crime and suspense'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 304,
     "status": "ok",
     "timestamp": 1730338035974,
     "user": {
      "displayName": "Marcela Flaibani",
      "userId": "06388299677795656548"
     },
     "user_tz": 180
    },
    "id": "_2hdzWDWg6HF",
    "outputId": "c9c90b2f-3591-4cde-9f61-27cbff214726"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Películas recomendadas:\n",
      "- Guardians of the Galaxy (Similitud: 0.84)\n",
      "- The Lego Movie (Similitud: 0.83)\n",
      "- Jurassic World (Similitud: 0.82)\n",
      "\n",
      "Juegos de mesa recomendados:\n",
      "- Dinosaur Island (Similitud: 0.81)\n",
      "- Jaws (Similitud: 0.80)\n",
      "- Shakespeare (Similitud: 0.79)\n",
      "\n",
      "Libros recomendados:\n",
      "- Twelfth Night; Or, What You Will (Similitud: 0.78)\n",
      "- The Importance of Being Earnest: A Trivial Comedy for Serious People (Similitud: 0.78)\n",
      "- A Princess of Mars (Similitud: 0.78)\n"
     ]
    }
   ],
   "source": [
    "# Función para obtener recomendaciones usando embeddings ya calculados\n",
    "def get_recommendations(precomputed_embeddings, user_preference, titles, top_n=3):\n",
    "    # Generar el embedding para la preferencia del usuario\n",
    "    user_embedding = model.encode(user_preference, convert_to_tensor=True)\n",
    "\n",
    "    # Calcular similitudes de coseno entre el embedding del usuario y las descripciones ya embebidas\n",
    "    cosine_similarities = util.cos_sim(user_embedding, precomputed_embeddings)[0]\n",
    "    similar_indices = cosine_similarities.argsort(descending=True)[:top_n].tolist()\n",
    "\n",
    "    # Extraer las recomendaciones\n",
    "    recommendations = [(titles.iloc[i], cosine_similarities[i].item()) for i in similar_indices]\n",
    "    return recommendations\n",
    "\n",
    "# Obtener recomendaciones usando los embeddings ya calculados\n",
    "movie_recommendations = get_recommendations(movie_embeddings, user_preferences_en, df_movies['Title'])\n",
    "game_recommendations = get_recommendations(game_embeddings, user_preferences_en, df_games['game_name'])\n",
    "book_recommendations = get_recommendations(book_embeddings, user_preferences_en, df_books['Title'])\n",
    "\n",
    "# Mostrar recomendaciones\n",
    "print(\"Películas recomendadas:\")\n",
    "for title, score in movie_recommendations:\n",
    "    print(f\"- {title} (Similitud: {score:.2f})\")\n",
    "\n",
    "print(\"\\nJuegos de mesa recomendados:\")\n",
    "for name, score in game_recommendations:\n",
    "    print(f\"- {name} (Similitud: {score:.2f})\")\n",
    "\n",
    "print(\"\\nLibros recomendados:\")\n",
    "for title, score in book_recommendations:\n",
    "    print(f\"- {title} (Similitud: {score:.2f})\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "U65MwcrBfPzo",
    "rrxpDK7Ae6iB",
    "fb7-pRmSfbLw",
    "RbNUHH5ofjNv",
    "TbUgsyB6Xu_x",
    "e0zKKEfQ2arq",
    "NlpzVzjorG9D"
   ],
   "provenance": [
    {
     "file_id": "1FYPdvojaoJ4JXoaWzOZQrZrbJix6o-iF",
     "timestamp": 1729983791964
    }
   ],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
